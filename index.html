
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="pyHaiCS Documentation.">
      
      
        <meta name="author" content="miguelfrndz">
      
      
        <link rel="canonical" href="https://pyhaics.github.io/">
      
      
      
        <link rel="next" href="quick_start/">
      
      
      <link rel="icon" href="img/feather-icon-purple.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>pyHaiCS Documentation</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="css/extra.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#welcome-to-pyhaics" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="pyHaiCS Documentation" class="md-header__button md-logo" aria-label="pyHaiCS Documentation" data-md-component="logo">
      
  <img src="img/feather-icon-white.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            pyHaiCS Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Home
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="deep-purple" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/miguelfrndz/pyHaiCS" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    miguelfrndz/pyHaiCS
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="pyHaiCS Documentation" class="md-nav__button md-logo" aria-label="pyHaiCS Documentation" data-md-component="logo">
      
  <img src="img/feather-icon-white.svg" alt="logo">

    </a>
    pyHaiCS Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/miguelfrndz/pyHaiCS" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    miguelfrndz/pyHaiCS
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pyhaics-features" class="md-nav__link">
    <span class="md-ellipsis">
      pyHaiCS Features
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-to-hamiltonian-monte-carlo" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction to Hamiltonian Monte-Carlo
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="benchmarks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Experimental Models
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pyhaics-features" class="md-nav__link">
    <span class="md-ellipsis">
      pyHaiCS Features
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-to-hamiltonian-monte-carlo" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction to Hamiltonian Monte-Carlo
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="welcome-to-pyhaics">Welcome to pyHaiCS!<a class="headerlink" href="#welcome-to-pyhaics" title="Permanent link">&para;</a></h1>
<div id="logo-welcome", align="center">
    <img src="img/logo.svg" alt="pyHaiCS Logo" style="height: auto; max-width: 55%; padding-bottom: 5%;">
</div>

<p>Introducing <code>pyHaiCS</code>, a Python library for <strong>Hamiltonian-based Monte-Carlo</strong> methods tailored towards practical applications in <em>computational statistics</em>. From sampling complex probability distributions, to approximating complex integrals, <code>pyHaiCS</code> is designed to be fast, flexible, and easy to use, with a focus on providing a <em>user-friendly</em> interface for researchers and practitioners while also offering users a variety of <strong>advanced features</strong>. </p>
<p>Although currently still in development, our library implements a wide range of <strong>sampling algorithms</strong> — including single-chain and multi-chain Hamiltoninan Monte-Carlo (HMC) and Generalized HMC (GHMC); a variety of numerical schemes for the <em>integration</em> of the simulated Hamiltonian dynamics (including a generalized version of Multi-Stage Splitting integrators), or a novel <em>adaptive</em> algorithm — Adaptive Integration Approach in Computational Statistics (s-AIA) — for the automatic tuning of the parameters of both the numerical integrator and the sampler. </p>
<p>Likewise, several utilities for <em>diagnosing</em> the convergence and efficiency of the sampling process, as well as <strong>multidisciplinary benchmarks</strong> — ranging from simple toy problems such as sampling from specific distributions, to more complex real-world applications in the fields of computational biology, Bayesian modeling, or physics — are provided.</p>
<h2 id="pyhaics-features">pyHaiCS Features<a class="headerlink" href="#pyhaics-features" title="Permanent link">&para;</a></h2>
<p>The main features of pyHaiCS, as summarized in the figure below, include its:</p>
<ul>
<li><strong>Efficient Implementation:</strong> <code>pyHaiCS</code> is built on top of the <code>JAX</code> library developed by Google which provides <strong>automatic differentiation</strong> for computing gradients and Hessians, and <strong>Just-In-Time (JIT) compilation</strong> for fast numerical computations. Additionally, the library is designed to take advantage of multi-core CPUs, GPUs, or even TPUs for <em>accelerated</em> sampling, and to be highly <em>parallelizable</em> (e.g., by running each chain of multi-chain HMC in a separate CPU core/thread in the GPU).</li>
<li><strong>User-Friendly Interface:</strong> The library is designed to be easy to use, with a simple and intuitive API that abstracts away the complexities of Hamiltonian Monte-Carlo (HMC) and related algorithms. Users can define their own potential functions and priors, and run sampling algorithms with just a few lines of code.</li>
<li><strong>Integration with Existing Tools:</strong> The library is designed to be <em>easily integrated</em> with other Python libraries, such as <code>NumPy</code>, <code>SciPy</code>, and <code>Scikit-Learn</code>. This allows users to leverage existing tools and workflows, and build on top of the rich ecosystem of scientific computing in Python. Therefore, users can easily incorporate <code>pyHaiCS</code> into their existing <strong>Machine Learning workflows</strong>, and use it for tasks such as inference, model selection, or parameter estimation in the context of Bayesian modeling.</li>
<li><strong>Advanced Features:</strong> <code>pyHaiCS</code> supports a variety of Hamiltonian-inspired sampling algorithms, including single-chain and multi-chain HMC (and GHMC), generalized <span class="arithmatex">\(k\)</span>-th stage Multi-Stage Splitting integrators, and adaptive integration schemes (such as s-AIA).</li>
</ul>
<div id="features", align="center">
    <img src="img/pyHaiCS_features.png" alt="pyHaiCS Features" style="height: auto; max-width: 90%; padding-bottom: 5%;">
</div>

<p>In order to provide a functional and <em>easy-to-use</em> library, and especially to ensure that our code can be easily integrated into existing workflows, we have designed <code>pyHaiCS</code> with a simple rule in mind: <strong>Objects are specified by interface, not by inheritance.</strong> That is, much alike <code>Scikit-Learn</code>, inheritance is <em>not enforced</em>; and instead, code conventions provide a <strong>consistent interface</strong> for all samplers, integrators, and utilities. This allows for a more flexible and modular design, and makes it easier for users to extend the library with their own custom implementations. As Scikit's design around making all estimators have a consistent <code>fit</code> and <code>predict</code> interface, <code>pyHaiCS</code> follows a similar approach, but with a focus on Hamiltonian Monte-Carlo methods and its related algorithms. For instance, all integrators in <code>pyHaiCS</code> have a consistent <code>integrate</code> method, which takes as input the potential function, the initial state, and the parameters of the integrator, and returns the final state of the system after the integration process. This consistent interface makes it easy for users to switch between different integrators, or to implement their own custom integrators, without having to worry about the underlying details of the implementation. </p>
<p>Moreover, <code>pyHaiCS</code> is designed to be highly <strong>modular</strong>, with each component of the library being self-contained and independent of the others, as well as being easily extensible and customizable. As a further point of strength, our library handles all <em>auto-differentiation</em> (such as potential gradients and Hessians) through the <code>JAX</code> library, which provides a fast and efficient way to compute gradients as well as a higher level of abstraction for the user to focus on the actual problem at hand. By only defining the <strong>potential</strong> function of the Hamiltonian, the user can easily run the sampler and obtain the posterior distribution of the parameters of interest. As an example of the <em>ease-of-use</em> of <code>pyHaiCS</code>, we show below a simple example of how to define a Bayesian Logistic Regression (BLR) model:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Step 1 - Define the BLR model</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">model_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>

<span class="c1"># Step 2 - Define the log-prior and log-likelihood</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">log_prior_fn</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">log_likelihood_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">model_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">preds</span><span class="p">))</span>

<span class="c1"># Step 3 - Define the log-posterior (remember, the oppositve of the potential)</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">log_posterior_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">log_prior_fn</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_likelihood_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<span class="c1"># Initialize the model parameters (including intercept term)</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">mean_vector</span><span class="p">,</span> <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">mean_vector</span><span class="p">,</span> <span class="n">cov_mat</span><span class="p">)</span>

<span class="c1"># HMC for posterior sampling</span>
<span class="n">params_samples</span> <span class="o">=</span> <span class="n">haics</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">hamiltonian</span><span class="o">.</span><span class="n">HMC</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> 
                            <span class="n">potential_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span>
                            <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">burn_in</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> 
                            <span class="n">step_size</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">n_steps</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> 
                            <span class="n">potential</span> <span class="o">=</span> <span class="n">neg_log_posterior_fn</span><span class="p">,</span>  
                            <span class="n">mass_matrix</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> 
                            <span class="n">integrator</span> <span class="o">=</span> <span class="n">haics</span><span class="o">.</span><span class="n">integrators</span><span class="o">.</span><span class="n">VerletIntegrator</span><span class="p">(),</span> 
                            <span class="n">RNG_key</span> <span class="o">=</span> <span class="n">key</span><span class="p">)</span>

<span class="c1"># Average across chains</span>
<span class="n">params_samples</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">params_samples</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Make predictions using the samples</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">model_fn</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">params</span><span class="p">))(</span><span class="n">params_samples</span><span class="p">)</span>
<span class="n">mean_preds</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p>Regarding the actual features implemented in <code>pyHaiCS</code>, and the general organization of its API, the figure below provides a <strong>high-level overview</strong> of the main components of the library. As can be seen, the library is organized around four main components: <em>Hamiltonian Samplers</em>, <em>Numerical Integrators</em>, <em>Adaptive Tuning</em>, and <em>Sampling Metrics</em>. Each of these components is further divided into sub-components, such as the different samplers implemented in the library (e.g., HMC, GHMC, and the yet to be implemented, MMHMC), the numerical integrators (such as variants of Velocity-Verlet, and  2-Stage and 3-Stage MSSIs), or the s-AIA adaptive tuning scheme. The library also includes a variety of sampling metrics for diagnosing the convergence and efficiency of the sampling process, as well as multidisciplinary benchmarks (and code examples) for testing the performance of the library.</p>
<div id="features-hierarchical", align="center">
    <img src="img/pyHaiCS_features_hierarchical.png" alt="pyHaiCS Features Hierarchical" style="height: auto; max-width: 90%; padding-bottom: 5%;">
</div>

<h2 id="introduction-to-hamiltonian-monte-carlo">Introduction to Hamiltonian Monte-Carlo<a class="headerlink" href="#introduction-to-hamiltonian-monte-carlo" title="Permanent link">&para;</a></h2>
<p>Markov-Chain Monte-Carlo (MCMC) methods are powerful tools for <strong>sampling</strong> from complex probability distributions, a task that lies at the heart of many statistical and Machine Learning problems. Among these, <strong>Hamiltonian Monte-Carlo (HMC)</strong> stands out as a particularly efficient and versatile algorithm, especially well-suited for high-dimensional problems.</p>
<p>Traditional MCMC methods, such as the Metropolis-Hastings algorithm or Gibbs sampling, often rely on random walk behavior to explore the target distribution. While effective, this can lead to slow convergence, especially when dealing with complex, multimodal, or high-dimensional distributions.  HMC addresses these limitations by introducing concepts from <strong>Hamiltonian dynamics</strong> to guide the exploration of the sample space.</p>
<p>At its core, HMC leverages the idea of simulating the movement of a particle in a physical system to generate efficient transitions across the target distribution <span class="arithmatex">\(\pi(\mathbf{q})\)</span>.  Let's break down the key elements:</p>
<ul>
<li>
<p><strong>Augmenting the State Space:</strong>  Imagine the probability distribution we want to sample from – our <strong>target distribution</strong> – as defining a potential energy landscape. Regions of high probability correspond to valleys (low potential energy), while regions of low probability are hills (high potential energy). To introduce dynamics, HMC <em>augments</em> our state space by adding <strong>auxiliary momentum variables</strong>, typically denoted as <span class="arithmatex">\(\mathbf{p}\)</span>, for each position variable <span class="arithmatex">\(\mathbf{q}\)</span> (our original parameters of interest).</p>
</li>
<li>
<p><strong>Hamiltonian Function:</strong> We then define a <strong>Hamiltonian function</strong>, <span class="arithmatex">\(H(\mathbf{q}, \mathbf{p})\)</span>, which describes the total energy of the system.  This function is typically the sum of two components:</p>
<ul>
<li><strong>Potential Energy</strong>, <span class="arithmatex">\(U(\mathbf{q})\)</span>:  This is directly related to our target probability distribution, <span class="arithmatex">\(\pi(\mathbf{q})\)</span>. Specifically, we often set <span class="arithmatex">\(U(\mathbf{q}) = -\log \pi(\mathbf{q})\)</span>.  Minimizing the potential energy corresponds to finding regions of high probability under <span class="arithmatex">\(\pi(\mathbf{q})\)</span>.</li>
<li><strong>Kinetic Energy</strong>, <span class="arithmatex">\(K(\mathbf{p})\)</span>: This term depends on the momentum variables and is usually defined as the energy of a fictitious "particle" associated with our system. A common choice is the kinetic energy of a particle with unit mass: <span class="arithmatex">\(K(\mathbf{p}) = \frac{1}{2} \mathbf{p}^T \mathbf{M}^{-1} \mathbf{p}\)</span>, where <span class="arithmatex">\(\mathbf{M}\)</span> is a mass matrix (often set to the identity matrix for simplicity).</li>
</ul>
<p>The Hamiltonian is then <span class="arithmatex">\(H(\mathbf{q}, \mathbf{p}) = U(\mathbf{q}) + K(\mathbf{p}) = -\log \pi(\mathbf{q}) + \frac{1}{2} \mathbf{p}^T \mathbf{M}^{-1} \mathbf{p}\)</span>.</p>
</li>
<li>
<p><strong>Hamilton's Equations of Motion:</strong>  The dynamics of the system are governed by <strong>Hamilton's equations of motion</strong>. These equations describe how the positions and momenta evolve over time:</p>
<div class="arithmatex">\[
\begin{aligned}
\frac{d\mathbf{q}}{dt} &amp;= \frac{\partial H}{\partial \mathbf{p}} = \mathbf{M}^{-1} \mathbf{p} \\
\frac{d\mathbf{p}}{dt} &amp;= -\frac{\partial H}{\partial \mathbf{q}} = -\frac{\partial U}{\partial \mathbf{q}} = \nabla \log \pi(\mathbf{q})
\end{aligned}
\]</div>
<p>These equations dictate that the "particle" will move through the potential energy landscape. Crucially, under these dynamics, the Hamiltonian <span class="arithmatex">\(H(\mathbf{q}, \mathbf{p})\)</span> (and thus the density <span class="arithmatex">\(\propto \exp(-H(\mathbf{q}, \mathbf{p}))\)</span>) remains constant over time in a continuous system.</p>
</li>
<li>
<p><strong>Numerical Integration:</strong>  To simulate these dynamics on a computer, we need to discretize time and use a <strong>numerical integrator</strong>.  <code>pyHaiCS</code> offers a variety of numerical integrators, including symplectic integrators, which are particularly well-suited for Hamiltonian systems because they preserve important properties of the dynamics, such as volume preservation and near-conservation of energy.</p>
</li>
<li>
<p><strong>Metropolis Acceptance Step:</strong>  While the Hamiltonian dynamics ideally preserve the target distribution, numerical integration introduces approximations, and thus trajectories are not perfectly Hamiltonian. To correct for these errors and ensure we are still sampling from the exact target distribution, HMC incorporates a <strong>Metropolis acceptance step</strong>. After evolving the system for a certain time using numerical integration, we compute the change in Hamiltonian, <span class="arithmatex">\(\Delta H\)</span>, between the start and end points of the trajectory.  We then accept the proposed new state with probability:</p>
<div class="arithmatex">\[
\alpha = \min\left(1, \exp(-\Delta H)\right) = \min\left(1, \exp(H(\mathbf{q}_{old}, \mathbf{p}_{old}) - H(\mathbf{q}_{new}, \mathbf{p}_{new}))\right)
\]</div>
<p>If the proposal is rejected, we simply retain the previous state. This acceptance step guarantees that the HMC algorithm samples from the correct target distribution, even with numerical integration approximations.</p>
</li>
</ul>
<p><strong>Benefits of HMC:</strong></p>
<p>By leveraging Hamiltonian dynamics, HMC offers several advantages over traditional MCMC methods:</p>
<ul>
<li><strong>Efficient Exploration of State Space:</strong> The Hamiltonian dynamics allows for more directed and less random-walk-like exploration of the target distribution.  Trajectories tend to follow gradients of the potential energy, enabling the sampler to move more quickly across the state space, especially in high dimensions.</li>
<li><strong>Reduced Random Walk Behavior:</strong>  Unlike algorithms relying on random proposals, HMC trajectories can travel significant distances in state space in each step, leading to faster convergence and more efficient sampling, particularly for distributions with complex geometries or long, narrow regions of high probability.</li>
<li><strong>Scalability to High Dimensions:</strong> The efficiency gains of HMC become more pronounced as the dimensionality of the problem increases, making it a powerful tool for complex statistical models with many parameters.</li>
</ul>
<p>In summary, Hamiltonian Monte Carlo provides a robust and efficient approach to MCMC sampling by harnessing the principles of Hamiltonian dynamics. By carefully simulating the physical movement of a system guided by the target distribution, HMC overcomes many limitations of traditional MCMC methods, enabling faster and more reliable sampling from complex, high-dimensional probability distributions.  <code>pyHaiCS</code> aims to make these powerful methods accessible and easy to use for a wide range of applications in <strong>computational statistics</strong> and <em>beyond</em> :)</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": ".", "features": [], "search": "assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>