{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to pyHaiCS!","text":"<p>Introducing <code>pyHaiCS</code>, a Python library for Hamiltonian-based Monte-Carlo methods tailored towards practical applications in computational statistics. From sampling complex probability distributions, to approximating complex integrals, <code>pyHaiCS</code> is designed to be fast, flexible, and easy to use, with a focus on providing a user-friendly interface for researchers and practitioners while also offering users a variety of advanced features. </p> <p>Although currently still in development, our library implements a wide range of sampling algorithms \u2014 including single-chain and multi-chain Hamiltoninan Monte-Carlo (HMC) and Generalized HMC (GHMC); a variety of numerical schemes for the integration of the simulated Hamiltonian dynamics (including a generalized version of Multi-Stage Splitting integrators), or a novel adaptive algorithm \u2014 Adaptive Integration Approach in Computational Statistics (s-AIA) \u2014 for the automatic tuning of the parameters of both the numerical integrator and the sampler. </p> <p>Likewise, several utilities for diagnosing the convergence and efficiency of the sampling process, as well as multidisciplinary benchmarks \u2014 ranging from simple toy problems such as sampling from specific distributions, to more complex real-world applications in the fields of computational biology, Bayesian modeling, or physics \u2014 are provided.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can use <code>pip</code> to install <code>pyHaiCS</code> from the GitHub official release builds. You can do this by running the following command in your terminal: <pre><code>pip install $(curl -s https://pyhaics.github.io/latest.txt)\n</code></pre> Note that the above command will install the latest official version of <code>pyHaiCS</code> currently available on the GitHub releases page.</p> <p>Alternatively, you can also install the library directly from the GitHub repository by running: <pre><code>pip install git+[url-to-pyHaiCS-repo]\n</code></pre> where <code>[url-to-pyHaiCS-repo]</code> is the URL to the <code>pyHaiCS</code> GitHub repository.</p>"},{"location":"#pyhaics-features","title":"pyHaiCS Features","text":"<p>The main features of pyHaiCS, as summarized in the figure below, include its:</p> <ul> <li>Efficient Implementation: <code>pyHaiCS</code> is built on top of the <code>JAX</code> library developed by Google which provides automatic differentiation for computing gradients and Hessians, and Just-In-Time (JIT) compilation for fast numerical computations. Additionally, the library is designed to take advantage of multi-core CPUs, GPUs, or even TPUs for accelerated sampling, and to be highly parallelizable (e.g., by running each chain of multi-chain HMC in a separate CPU core/thread in the GPU).</li> <li>User-Friendly Interface: The library is designed to be easy to use, with a simple and intuitive API that abstracts away the complexities of Hamiltonian Monte-Carlo (HMC) and related algorithms. Users can define their own potential functions and priors, and run sampling algorithms with just a few lines of code.</li> <li>Integration with Existing Tools: The library is designed to be easily integrated with other Python libraries, such as <code>NumPy</code>, <code>SciPy</code>, and <code>Scikit-Learn</code>. This allows users to leverage existing tools and workflows, and build on top of the rich ecosystem of scientific computing in Python. Therefore, users can easily incorporate <code>pyHaiCS</code> into their existing Machine Learning workflows, and use it for tasks such as inference, model selection, or parameter estimation in the context of Bayesian modeling.</li> <li>Advanced Features: <code>pyHaiCS</code> supports a variety of Hamiltonian-inspired sampling algorithms, including single-chain and multi-chain HMC (and GHMC), generalized \\(k\\)-th stage Multi-Stage Splitting integrators, and adaptive integration schemes (such as s-AIA).</li> </ul> <p>In order to provide a functional and easy-to-use library, and especially to ensure that our code can be easily integrated into existing workflows, we have designed <code>pyHaiCS</code> with a simple rule in mind: Objects are specified by interface, not by inheritance. That is, much alike <code>Scikit-Learn</code>, inheritance is not enforced; and instead, code conventions provide a consistent interface for all samplers, integrators, and utilities. This allows for a more flexible and modular design, and makes it easier for users to extend the library with their own custom implementations. As Scikit's design around making all estimators have a consistent <code>fit</code> and <code>predict</code> interface, <code>pyHaiCS</code> follows a similar approach, but with a focus on Hamiltonian Monte-Carlo methods and its related algorithms. For instance, all integrators in <code>pyHaiCS</code> have a consistent <code>integrate</code> method, which takes as input the potential function, the initial state, and the parameters of the integrator, and returns the final state of the system after the integration process. This consistent interface makes it easy for users to switch between different integrators, or to implement their own custom integrators, without having to worry about the underlying details of the implementation. </p> <p>Moreover, <code>pyHaiCS</code> is designed to be highly modular, with each component of the library being self-contained and independent of the others, as well as being easily extensible and customizable. As a further point of strength, our library handles all auto-differentiation (such as potential gradients and Hessians) through the <code>JAX</code> library, which provides a fast and efficient way to compute gradients as well as a higher level of abstraction for the user to focus on the actual problem at hand. By only defining the potential function of the Hamiltonian, the user can easily run the sampler and obtain the posterior distribution of the parameters of interest. As an example of the ease-of-use of <code>pyHaiCS</code>, we show below a simple example of how to define a Bayesian Logistic Regression (BLR) model:</p> <pre><code># Step 1 - Define the BLR model\n@jax.jit\ndef model_fn(x, params):\n    return jax.nn.sigmoid(jnp.matmul(x, params))\n\n# Step 2 - Define the log-prior and log-likelihood\n@jax.jit\ndef log_prior_fn(params):\n    return jnp.sum(jax.scipy.stats.norm.logpdf(params))\n\n@jax.jit\ndef log_likelihood_fn(x, y, params):\n    preds = model_fn(x, params)\n    return jnp.sum(y * jnp.log(preds) + (1 - y) * jnp.log(1 - preds))\n\n# Step 3 - Define the log-posterior (remember, the oppositve of the potential)\n@jax.jit\ndef log_posterior_fn(x, y, params):\n    return log_prior_fn(params) + log_likelihood_fn(x, y, params)\n\n# Initialize the model parameters (including intercept term)\nkey = jax.random.PRNGKey(42)\nmean_vector, cov_mat = jnp.zeros(X_train.shape[1]), jnp.eye(X_train.shape[1])\nparams = jax.random.multivariate_normal(key, mean_vector, cov_mat)\n\n# HMC for posterior sampling\nparams_samples = haics.samplers.hamiltonian.HMC(params, \n                            potential_args = (X_train, y_train),\n                            n_samples = 1000, burn_in = 200, \n                            step_size = 1e-3, n_steps = 100, \n                            potential = neg_log_posterior_fn,  \n                            mass_matrix = jnp.eye(X_train.shape[1]), \n                            integrator = haics.integrators.VerletIntegrator(), \n                            RNG_key = key)\n\n# Average across chains\nparams_samples = jnp.mean(params_samples, axis = 0)\n\n# Make predictions using the samples\npreds = jax.vmap(lambda params: model_fn(X_test, params))(params_samples)\nmean_preds = jnp.mean(preds, axis = 0)\n</code></pre> <p>Regarding the actual features implemented in <code>pyHaiCS</code>, and the general organization of its API, the figure below provides a high-level overview of the main components of the library. As can be seen, the library is organized around four main components: Hamiltonian Samplers, Numerical Integrators, Adaptive Tuning, and Sampling Metrics. Each of these components is further divided into sub-components, such as the different samplers implemented in the library (e.g., HMC, GHMC, and the yet to be implemented, MMHMC), the numerical integrators (such as variants of Velocity-Verlet, and  2-Stage and 3-Stage MSSIs), or the s-AIA adaptive tuning scheme. The library also includes a variety of sampling metrics for diagnosing the convergence and efficiency of the sampling process, as well as multidisciplinary benchmarks (and code examples) for testing the performance of the library.</p>"},{"location":"#introduction-to-hamiltonian-monte-carlo","title":"Introduction to Hamiltonian Monte-Carlo","text":"<p>Markov-Chain Monte-Carlo (MCMC) methods are powerful tools for sampling from complex probability distributions, a task that lies at the heart of many statistical and Machine Learning problems. Among these, Hamiltonian Monte-Carlo (HMC) stands out as a particularly efficient and versatile algorithm, especially well-suited for high-dimensional problems.</p> <p>Traditional MCMC methods, such as the Metropolis-Hastings algorithm or Gibbs sampling, often rely on random walk behavior to explore the target distribution. While effective, this can lead to slow convergence, especially when dealing with complex, multimodal, or high-dimensional distributions.  HMC addresses these limitations by introducing concepts from Hamiltonian dynamics to guide the exploration of the sample space.</p> <p>At its core, HMC leverages the idea of simulating the movement of a particle in a physical system to generate efficient transitions across the target distribution \\(\\pi(\\mathbf{q})\\).  Let's break down the key elements:</p> <ul> <li> <p>Augmenting the State Space:  Imagine the probability distribution we want to sample from \u2013 our target distribution \u2013 as defining a potential energy landscape. Regions of high probability correspond to valleys (low potential energy), while regions of low probability are hills (high potential energy). To introduce dynamics, HMC augments our state space by adding auxiliary momentum variables, typically denoted as \\(\\mathbf{p}\\), for each position variable \\(\\mathbf{q}\\) (our original parameters of interest).</p> </li> <li> <p>Hamiltonian Function: We then define a Hamiltonian function, \\(H(\\mathbf{q}, \\mathbf{p})\\), which describes the total energy of the system.  This function is typically the sum of two components:</p> <ul> <li>Potential Energy, \\(U(\\mathbf{q})\\):  This is directly related to our target probability distribution, \\(\\pi(\\mathbf{q})\\). Specifically, we often set \\(U(\\mathbf{q}) = -\\log \\pi(\\mathbf{q})\\).  Minimizing the potential energy corresponds to finding regions of high probability under \\(\\pi(\\mathbf{q})\\).</li> <li>Kinetic Energy, \\(K(\\mathbf{p})\\): This term depends on the momentum variables and is usually defined as the energy of a fictitious \"particle\" associated with our system. A common choice is the kinetic energy of a particle with unit mass: \\(K(\\mathbf{p}) = \\frac{1}{2} \\mathbf{p}^T \\mathbf{M}^{-1} \\mathbf{p}\\), where \\(\\mathbf{M}\\) is a mass matrix (often set to the identity matrix for simplicity).</li> </ul> <p>The Hamiltonian is then \\(H(\\mathbf{q}, \\mathbf{p}) = U(\\mathbf{q}) + K(\\mathbf{p}) = -\\log \\pi(\\mathbf{q}) + \\frac{1}{2} \\mathbf{p}^T \\mathbf{M}^{-1} \\mathbf{p}\\).</p> </li> <li> <p>Hamilton's Equations of Motion:  The dynamics of the system are governed by Hamilton's equations of motion. These equations describe how the positions and momenta evolve over time:</p> \\[ \\begin{aligned} \\frac{d\\mathbf{q}}{dt} &amp;= \\frac{\\partial H}{\\partial \\mathbf{p}} = \\mathbf{M}^{-1} \\mathbf{p} \\\\ \\frac{d\\mathbf{p}}{dt} &amp;= -\\frac{\\partial H}{\\partial \\mathbf{q}} = -\\frac{\\partial U}{\\partial \\mathbf{q}} = \\nabla \\log \\pi(\\mathbf{q}) \\end{aligned} \\] <p>These equations dictate that the \"particle\" will move through the potential energy landscape. Crucially, under these dynamics, the Hamiltonian \\(H(\\mathbf{q}, \\mathbf{p})\\) (and thus the density \\(\\propto \\exp(-H(\\mathbf{q}, \\mathbf{p}))\\)) remains constant over time in a continuous system.</p> </li> <li> <p>Numerical Integration:  To simulate these dynamics on a computer, we need to discretize time and use a numerical integrator.  <code>pyHaiCS</code> offers a variety of numerical integrators, including symplectic integrators, which are particularly well-suited for Hamiltonian systems because they preserve important properties of the dynamics, such as volume preservation and near-conservation of energy.</p> </li> <li> <p>Metropolis Acceptance Step:  While the Hamiltonian dynamics ideally preserve the target distribution, numerical integration introduces approximations, and thus trajectories are not perfectly Hamiltonian. To correct for these errors and ensure we are still sampling from the exact target distribution, HMC incorporates a Metropolis acceptance step. After evolving the system for a certain time using numerical integration, we compute the change in Hamiltonian, \\(\\Delta H\\), between the start and end points of the trajectory.  We then accept the proposed new state with probability:</p> \\[ \\alpha = \\min\\left(1, \\exp(-\\Delta H)\\right) = \\min\\left(1, \\exp(H(\\mathbf{q}_{old}, \\mathbf{p}_{old}) - H(\\mathbf{q}_{new}, \\mathbf{p}_{new}))\\right) \\] <p>If the proposal is rejected, we simply retain the previous state. This acceptance step guarantees that the HMC algorithm samples from the correct target distribution, even with numerical integration approximations.</p> </li> </ul> <p>Benefits of HMC:</p> <p>By leveraging Hamiltonian dynamics, HMC offers several advantages over traditional MCMC methods:</p> <ul> <li>Efficient Exploration of State Space: The Hamiltonian dynamics allows for more directed and less random-walk-like exploration of the target distribution.  Trajectories tend to follow gradients of the potential energy, enabling the sampler to move more quickly across the state space, especially in high dimensions.</li> <li>Reduced Random Walk Behavior:  Unlike algorithms relying on random proposals, HMC trajectories can travel significant distances in state space in each step, leading to faster convergence and more efficient sampling, particularly for distributions with complex geometries or long, narrow regions of high probability.</li> <li>Scalability to High Dimensions: The efficiency gains of HMC become more pronounced as the dimensionality of the problem increases, making it a powerful tool for complex statistical models with many parameters.</li> </ul> <p>In summary, Hamiltonian Monte Carlo provides a robust and efficient approach to MCMC sampling by harnessing the principles of Hamiltonian dynamics. By carefully simulating the physical movement of a system guided by the target distribution, HMC overcomes many limitations of traditional MCMC methods, enabling faster and more reliable sampling from complex, high-dimensional probability distributions.  <code>pyHaiCS</code> aims to make these powerful methods accessible and easy to use for a wide range of applications in computational statistics and beyond :)</p>"},{"location":"api/","title":"pyHaiCS API Documentation","text":"<p>In this section, we provide a detailed API reference for the <code>pyHaiCS</code> library. The library is organized around four main components: Hamiltonian Samplers, Numerical Integrators, Adaptive Tuning, and Sampling Metrics. Each of these components is further divided into sub-components, such as the different samplers implemented in the library (e.g., HMC, GHMC, and the yet to be implemented, MMHMC), the numerical integrators (such as variants of Velocity-Verlet, and  2-Stage and 3-Stage MSSIs), or the s-AIA adaptive tuning scheme. The library also includes a variety of sampling metrics for diagnosing the convergence and efficiency of the sampling process, as well as multidisciplinary benchmarks for testing the performance of the library.</p>"},{"location":"api/#samplers","title":"Samplers","text":"<p>Samplers are the main components of the library. They implement the Hamiltonian Monte Carlo (HMC) and Generalized Hamiltonian Monte Carlo (GHMC) algorithms, both in their standard and adaptive versions. Moreover, both single-chain and multi-chain versions are implemented.</p>"},{"location":"api/#hamiltonian-monte-carlo-hmc","title":"Hamiltonian Monte Carlo (HMC)","text":"<pre><code>def HMC(x_init, potential_args, n_samples, burn_in, step_size,\n        n_steps, potential, mass_matrix, integrator, n_chains, RNG_key)\n</code></pre> <p>Standard Hamiltonian Monte-Carlo (HMC) sampler.</p> <p>Algorithm:</p> <pre><code>1. Initialize x\u2080\n2. For i = 1 to n_samples:\n    - Draw momentum p ~ N(0, M)  \n    - Simulate Hamiltonian dynamics for n_steps:\n        (x*, p*) = Integrator(x, p, step_size, n_steps)\n    - Accept (x*, p*) with probability:\n        min(1, exp(H(x,p) - H(x*,p*)))\n    - Store x* (if accepted)\n    - Discard the momentum p\n3. Return samples after burn-in\n</code></pre> <p>Parameters:</p> <ul> <li><code>x_init</code>: Initial position</li> <li><code>potential_args</code>: Arguments for the potential function (e.g., training data for Bayesian models)</li> <li><code>n_samples</code>: Number of samples to generate</li> <li><code>burn_in</code>: Number of burn-in samples</li> <li><code>step_size</code>: Integration step-size</li> <li><code>n_steps</code>: Number of integration steps per proposal</li> <li><code>potential</code>: Hamiltonian potential function</li> <li><code>mass_matrix</code>: Mass matrix for the Hamiltonian dynamics</li> <li><code>integrator</code>: Numerical integrator (default: VerletIntegrator)</li> <li><code>n_chains</code>: Number of parallel chains (default: 4)</li> <li><code>RNG_key</code>: Random number generator key (default: 42)</li> </ul> <p>Returns:</p> <ul> <li><code>samples</code>: Array of samples (multiple chains)</li> </ul>"},{"location":"api/#generalized-hamiltonian-monte-carlo-ghmc","title":"Generalized Hamiltonian Monte Carlo (GHMC)","text":"<pre><code>def GHMC(x_init, potential_args, n_samples, burn_in, step_size,\n         n_steps, potential, mass_matrix, momentum_noise, integrator, \n         n_chains, RNG_key)\n</code></pre> <p>Generalized Hamiltonian Monte-Carlo (GHMC) sampler with momentum updates.</p> <p>Algorithm:</p> <pre><code>1. Initialize x\u2080, p\u2080\n2. For i = 1 to n_samples:\n    - Draw mu ~ N(0, M)\n    - Propose updated momentum p' = sqrt(1-phi)*p + sqrt(phi)*mu\n    - Propose new noise vector mu' = sqrt(1-phi)*mu + sqrt(phi)*p\n    - Simulate Hamiltonian dynamics for n_steps:\n        (x*, p*) = Integrator(x, p', step_size, n_steps)\n    - Accept (x*, p*) with probability:\n        min(1, exp(H(x,p') - H(x*,p*)))\n    - Store (x*, p') (if accepted)\n    - Otherwise, perform a momentum flip: (x, p) = (x, -p')\n3. Return samples after burn-in\n</code></pre> <p>Parameters:</p> <ul> <li>Same as HMC, plus:</li> <li><code>momentum_noise</code>: Noise parameter for momentum resampling</li> </ul> <p>Returns:</p> <ul> <li><code>samples</code>: Array of samples (multiple chains)</li> </ul>"},{"location":"api/#numerical-integrators","title":"Numerical Integrators","text":"<p>Likewise, numerous numerical integrators to simulate the Hamiltonian dynamics are implemented. Important note: All numerical integrators are implemented as classes that inherit from the <code>Integrator</code> class and have a common interface. All samplers call the <code>integrate()</code> method of the integrator class to simulate the Hamiltonian dynamics.</p>"},{"location":"api/#leapfrogmodified-1-stage-verlet-integrator-default","title":"Leapfrog/Modified 1-Stage Verlet Integrator (Default)","text":"<p>This is the default integrator used by pyHaiCS. It is a modified 1-stage Verlet integrator with momentum half-steps.</p> <pre><code>integrator = VerletIntegrator()\n</code></pre> <p>Algorithm:</p> <pre><code>1. Update momentum (half-step): p = p - step_size/2 * potential_grad(x)\n2. For i = 1 to (n_integration_steps - 1):\n    - Update position (full-step): x = x + step_size * M^(-1) * p\n    - Update momentum (full-step): p = p - step_size * potential_grad(x)\n3. Update position (full-step): x = x + step_size * M^(-1) * p\n4. Update momentum (half-step): p = p - step_size/2 * potential_grad(x)\n5. Return x, p\n</code></pre>"},{"location":"api/#multi-stage-splitting-integrators-mssis","title":"Multi-Stage Splitting Integrators (MSSIs)","text":"<p>The library implements various multi-stage splitting integrators for simulating Hamiltonian dynamics.</p> Integrator N\u00ba of Stages (\\(k\\)) Coefficients 1-Stage Velocity Verlet (VV1) 1 - 2-Stage Velocity Verlet (VV2) 2 \\(b = 1/4\\) 2-Stage BCSS (BCSS2) 2 \\(b = 0.211781\\) 2-Stage Minimum-Error (ME2) 2 \\(b = 0.193183\\) 3-Stage Velocity Verlet (VV3) 3 \\(a = 1/3, b = 1/6\\) 3-Stage BCSS (BCSS3) 3 \\(a = 0.296195, b = 0.118880\\) 3-Stage Minimum-Error (ME3) 3 \\(a = 0.290486, b = 0.108991\\)"},{"location":"api/#second-stage-mssis","title":"Second-Stage MSSIs","text":"<pre><code>integrator = MSSI_2(b)\n</code></pre> <p>Base class for second-stage multi-stage splitting integrators.</p> <p>Available implementations:</p> <ul> <li><code>VV_2</code>: Velocity-Verlet integrator (\\(b = 1/4\\))</li> <li><code>BCSS_2</code>: BCSS integrator (\\(b = 0.211781\\))</li> <li><code>ME_2</code>: Minimum Error integrator (\\(b = 0.193183\\))</li> </ul>"},{"location":"api/#third-stage-mssis","title":"Third-Stage MSSIs","text":"<pre><code>integrator = MSSI_3(a, b)\n</code></pre> <p>Base class for third-stage multi-stage splitting integrators.</p> <p>Available implementations:</p> <ul> <li><code>VV_3</code>: Velocity-Verlet integrator (\\(a = 1/3, b = 1/6\\))</li> <li><code>BCSS_3</code>: BCSS integrator (\\(a = 0.296195, b = 0.118880\\))</li> <li><code>ME_3</code>: Minimum Error integrator (\\(a = 0.290486, b = 0.108991\\))</li> </ul>"},{"location":"api/#adaptive-tuning-methods","title":"Adaptive Tuning Methods","text":""},{"location":"api/#statistical-adaptive-integration-approach-s-aia","title":"Statistical Adaptive Integration Approach (s-AIA)","text":"<p>Moreover, our library implements novel adaptive methods, such as s-AIA, for automatically tuning the parameters of the numerical integrator and the sampler. This algorithm is particularly useful for applications in computational statistics where manual tuning of parameters can be time-consuming and error-prone.</p> <p>Importantly, the s-AIA algorithm in pyHaiCS is designed to be used as if it were a sampler, i.e., it can be called with the same syntax as the other samplers implemented in the library.</p> <pre><code>def sAIA(x_init, potential_args, n_samples_tune, n_samples_check, \n         n_samples_burn_in, n_samples_prod, potential, mass_matrix, \n         target_AR, stage, sensibility, delta_step, compute_freqs, \n         sampler, RNG_key)\n</code></pre> <p>The s-AIA method works by iteratively optimizing the integration parameters to achieve a target acceptance rate while maintaining the efficiency of the sampling process. It consists of three main phases:</p> <ol> <li> <p>Tuning Phase: During this phase, the algorithm explores different combinations of step sizes and integration steps to find optimal values that lead to the desired acceptance rate.</p> </li> <li> <p>Burn-in Phase: Once the parameters are tuned, this phase allows the sampler to converge to the target distribution while maintaining the optimized parameters.</p> </li> <li> <p>Production Phase: The final phase where samples are collected using the optimized parameters.</p> </li> </ol> <p>The algorithm is particularly effective because it:</p> <ul> <li>Optimizes both the parameters of the numerical integrator and the sampler.</li> <li>Can be used with both HMC and GHMC samplers.</li> <li>Provides optimal coefficients for multi-stage splitting integrators.</li> <li>Includes momentum noise optimization for GHMC.</li> <li>Removes the need for manual tuning any parameters or running multiple chains.</li> </ul> <p>Parameters:</p> <ul> <li><code>x_init</code>: Initial position</li> <li><code>potential_args</code>: Arguments for the potential function</li> <li><code>n_samples_tune</code>: Number of samples to tune the parameters</li> <li><code>n_samples_check</code>: Number of samples to check the convergence</li> <li><code>n_samples_burn_in</code>: Number of samples for burn-in</li> <li><code>n_samples_prod</code>: Number of samples for production</li> <li><code>potential</code>: Hamiltonian potential function</li> <li><code>mass_matrix</code>: Mass matrix for the Hamiltonian dynamics</li> <li><code>target_AR</code>: Target acceptance rate</li> <li><code>stage</code>: Number of stages for the multi-stage splitting integrator</li> <li><code>sensibility</code>: Sensibility parameter for the s-AIA algorithm</li> <li><code>delta_step</code>: Step size for the parameter search</li> <li><code>compute_freqs</code>: Whether to compute the frequencies of the potential</li> <li><code>sampler</code>: Sampler (default: HMC)</li> <li><code>RNG_key</code>: Random number generator key</li> </ul> <p>Returns:</p> <ul> <li><code>samples</code>: Array of samples</li> </ul>"},{"location":"api/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>The library provides a comprehensive set of metrics for evaluating the performance and convergence of the samplers. These metrics are essential for diagnosing the quality of the sampling process and ensuring reliable results.</p> <p>The <code>compute_metrics()</code> function can be used to compute all the metrics implemented in the library for a given set of samples.</p> <pre><code>def compute_metrics(samples, thres_estimator, normalize_ESS)\n</code></pre> <p>Parameters:</p> <ul> <li><code>samples</code>: Array of samples</li> <li><code>thres_estimator</code>: Threshold estimator for the effective sample size (ESS)</li> <li><code>normalize_ESS</code>: Whether to normalize the ESS values</li> </ul>"},{"location":"api/#acceptance-rate","title":"Acceptance Rate","text":"<pre><code>def acceptance_rate(num_acceptals, n_samples)\n</code></pre> <p>Computes the acceptance rate from a sequence of accepted/rejected proposals.</p> <p>Parameters:</p> <ul> <li><code>num_acceptals</code>: Number of accepted proposals</li> <li><code>n_samples</code>: Total number of proposals</li> </ul> <p>Returns:</p> <ul> <li><code>float</code>: Acceptance rate between 0 and 1</li> </ul>"},{"location":"api/#rejection-rate","title":"Rejection Rate","text":"<pre><code>def rejection_rate(num_acceptals, n_samples)\n</code></pre> <p>Computes the rejection rate from a sequence of accepted/rejected proposals.</p> <p>Parameters:</p> <ul> <li><code>num_acceptals</code>: Number of accepted proposals</li> <li><code>n_samples</code>: Total number of proposals</li> </ul> <p>Returns:</p> <ul> <li><code>float</code>: Rejection rate between 0 and 1</li> </ul>"},{"location":"api/#potential-scale-reduction-factor-psrf","title":"Potential Scale Reduction Factor (PSRF)","text":"<pre><code>def PSRF(samples)\n</code></pre> <p>Computes the potential scale reduction factor (Gelman-Rubin diagnostic) for multiple chains.</p> <p>Parameters:</p> <ul> <li><code>samples</code>: Array of samples from multiple chains</li> </ul> <p>Returns:</p> <ul> <li><code>float</code>: PSRF value</li> </ul> <p>Note: A PSRF close to 1 indicates good convergence across chains.</p>"},{"location":"api/#effective-sample-size-ess","title":"Effective Sample Size (ESS)","text":"<p>Geyer's initial Markov Chain Monte Carlo (MCE) estimator is implemented in the library.</p> <p>To be completed...</p>"},{"location":"api/#monte-carlo-standard-error-mcse","title":"Monte-Carlo Standard Error (MCSE)","text":"<pre><code>def MCSE(samples, ess_values)\n</code></pre> <p>Computes the Monte Carlo Standard Error (MCSE) from the effective sample size (ESS).</p> <p>Parameters:</p> <ul> <li><code>samples</code>: Array of samples from the chain</li> <li><code>ess_values</code>: Effective sample size values</li> </ul> <p>Returns:</p> <ul> <li><code>float</code>: Monte Carlo Standard Error</li> </ul> <p>Note: The MCSE is a measure of the precision of the estimator, it is inversely proportional to the square root of the ESS.</p>"},{"location":"api/#integrated-autocorrelation-time-iact","title":"Integrated Autocorrelation Time (IACT)","text":"<pre><code>def IACT(samples, ess_values, normalized_ESS = True)\n</code></pre> <p>Computes the integrated autocorrelation time, which measures the autocorrelation between samples, it can also be defined as the number of Monte-Carlo iterations needed, on average, for an independent sample to be drawn.</p> <p>Parameters:</p> <ul> <li><code>samples</code>: Array of samples from the chain</li> <li><code>ess_values</code>: Effective sample size values</li> <li><code>normalized_ESS</code>: Whether to normalize the ESS values</li> </ul> <p>Returns:</p> <ul> <li><code>float</code>: Integrated autocorrelation time</li> </ul> <p>Note: On average, IACT correlated samples are required in order to reduce the variance of the estimator by the same amount as a single uncorrelated sample.</p>"},{"location":"api/#utility-functions","title":"Utility Functions","text":""},{"location":"api/#kinetic-energy","title":"Kinetic Energy","text":"<pre><code>@jax.jit\ndef Kinetic(p, mass_matrix)\n</code></pre> <p>Computes the kinetic energy for given momentum and mass matrix.</p>"},{"location":"api/#hamiltonian","title":"Hamiltonian","text":"<pre><code>def Hamiltonian(x, p, potential, mass_matrix)\n</code></pre> <p>Computes the total Hamiltonian energy.</p>"},{"location":"benchmarks/","title":"Experimental Benchmark Models","text":"<p>This section presents a collection of benchmarking scenarios for comparing and assessing the performance, and behavior, of different Hamiltonian-based estimators based on several of their intrinsic properties:</p> <ol> <li>Correlation in the samples.</li> <li>Reversibility of the chain.</li> <li>Influence of the Importance Sampling re-weighting (e.g., for MMHMC).</li> </ol> <p>Importantly, all necessary data (e.g., datasets, parameters, etc.) for running the experiments is provided in the <code>becnhmarks/</code> directory.</p>"},{"location":"benchmarks/#banana-shaped-distribution","title":"Banana-Shaped Distribution","text":"<p>Given data \\(\\{y_k\\}_{k=1}^K\\), we sample from the banana-shaped posterior distribution of the parameter \\(\\theta = (\\theta_1, \\theta_2)\\) for which the likelihood and prior distributions are respectively given as:</p> \\[ \\begin{aligned} y_k|\\theta &amp;\\sim \\mathcal{N}(\\theta_1+\\theta_2^2, \\sigma_y^2), \\quad k=1, 2, \\ldots, K\\\\ \\theta_1,\\theta_2 &amp;\\sim \\mathcal{N}(0,\\sigma_{\\theta}^2) \\end{aligned} \\] <p>The sample data are generated with \\(\\theta_1+\\theta_2^2=1, \\sigma_y=2, \\sigma_{\\theta}=1\\). Then, the potential function is given by:</p> \\[ U(\\theta)=\\dfrac{1}{2\\sigma_y^2}\\sum_{k=1}^K (y_k - \\theta_1 - \\theta_2^2)^2 + \\log \\left(\\sigma_\\theta^2\\sigma_y^{100}\\right)+\\dfrac{1}{2\\sigma_\\theta^2}(\\theta_1^2 + \\theta_2^2) \\]"},{"location":"benchmarks/#multivariate-gaussian-distribution","title":"Multivariate Gaussian Distribution","text":"<p>We sample from a \\(D\\)-dimensional Multivariate Gaussian Distribution \\(\\mathcal{N} (0, \\Sigma)\\), where the precision matrix \\(\\Sigma^{-1}\\) is generated from a Wishart distribution.</p> <p>For computational purposes, we take \\(D=1000\\) dimensions and the covariance matrix to be diagonal with:</p> \\[ \\Sigma_{ii}=\\sigma_i^2 \\] <p>where \\(\\sigma_i^2\\) is the \\(i\\)-th smallest eigenvalue of the original covariance matrix. The potential function in this case is defined as:</p> \\[ U(\\theta)=\\dfrac{1}{2}\\theta^T \\Sigma^{-1}\\theta \\]"},{"location":"benchmarks/#bayesian-logistic-regression-blr","title":"Bayesian Logistic Regression (BLR)","text":"<p>Bayesian Logistic Regression (BLR) is the probabilistic extension of the traditional point-estimate logistic regression model by incorporating a prior distribution over the parameters of the model.</p> <p>Given \\(K\\) data instances \\(\\{x_k, y_k\\}_{k=1}^K\\) where \\(x_k=(1, x_1, \\ldots, x_D)\\) are vectors of \\(D\\) covariates and \\(y_k \\in \\{0, 1\\}\\) are the binary responses, the probability of a particular outcome is linked to the linear predictor function through the logit function:</p> \\[ \\begin{aligned} p(y_k| x_k, \\theta) &amp;= \\sigma(\\theta^Tx_k) = \\dfrac{1}{1+\\exp(-\\theta^Tx_k)}\\\\ \\theta^Tx_k\\equiv \\operatorname{logit}(p_k) &amp;= \\log \\left(\\dfrac{p_k}{1-p_k}\\right)=\\theta_0 + \\theta_1 x_{1,k}+\\ldots \\theta_D x_{D, k} \\end{aligned} \\] <p>where \\(\\theta=(\\theta_0, \\theta_1, \\ldots, \\theta_D)^T\\) are the parameters of the model, with \\(\\theta_0\\) denoted as the intercept.</p> <p>The prior distribution over the parameters \\(\\theta\\) is chosen to be a Multivariate Gaussian distribution:</p> \\[ \\theta \\sim \\mathcal{N}(\\mu, \\Sigma), \\quad \\text{Usually } \\theta \\sim \\mathcal{N}(0, I_{D+1}) \\] <p>where \\(\\mu\\in \\mathbb{R}^{D+1}\\) is the mean vector, \\(\\Sigma \\in \\mathbb{R}^{D+1}\\) is the covariance matrix, \\(0\\) is the zero vector and \\(I_{D+1}\\) is the identity matrix of order \\(D+1\\).</p> <p>In order to simplify the notation, let us define the vectorized response variable \\(\\symbf{y}=(y_1, \\ldots, y_K)\\), and the design matrix \\(X\\in \\mathbb{R}^{K, D}\\) as the input to the model:</p> \\[     X = \\begin{pmatrix}     1 &amp; x_{1,1} &amp; \\ldots &amp; x_{1,D}\\\\     1 &amp; x_{2,1} &amp; \\ldots &amp; x_{2,D}\\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\     1 &amp; x_{K,1} &amp; \\ldots &amp; x_{K,D}     \\end{pmatrix} \\] <p>The likelihood of the data is given by the product of the Bernoulli distributions as:</p> \\[ \\mathcal{L}(\\symbf{y}\\vert X, \\symbf{\\theta}) \\equiv p(\\symbf{y}\\vert X, \\symbf{\\theta})=\\prod_{k=1}^K p(y_k\\vert X_k, \\symbf{\\theta})=\\prod_{k=1}^K \\left(\\dfrac{\\exp(X_k\\symbf{\\theta})}{1+\\exp(X_k\\symbf{\\theta})}\\right)^{y_k}\\left(\\dfrac{1}{1+\\exp(X_k\\symbf{\\theta})}\\right)^{1-y_k} \\] <p>where \\(X_k=(1, x_{k, 1}, \\ldots, x_{k,D})\\) is the \\(k\\)-th entry row vector of the design matrix \\(X\\).</p> <p>Then, the potential function can be expressed as:</p> \\[ U(\\symbf{\\theta})=-\\sum_{k=1}^K \\left[ y_k \\cdot X_k\\symbf{\\theta} - \\log \\left(1+\\exp(X_k\\symbf{\\theta})\\right)\\right] + \\dfrac{1}{2\\alpha} \\sum_{i=1}^D \\theta_i^2 \\]"},{"location":"benchmarks/#available-datasets","title":"Available Datasets","text":"<p>The following datasets are included for benchmarking the BLR model:</p> Dataset D K Reference German 25 1000 German Credit Dataset Sonar 61 208 Sonar Dataset Musk 167 476 Musk Dataset (Version 1) Secom 444 1567 SECOM Dataset"},{"location":"benchmarks/#dynamic-covid-19-epidemiological-models","title":"Dynamic COVID-19 Epidemiological Models","text":"<p>A SEIR (Susceptible-Exposed-Infectious-Remove) dynamic compartmental mechanistic epidemiological model with a time-dependent transmission rate parametrized using Bayesian P-splines is applied to modeling the COVID-19 incidence data in the Basque Country (Spain).</p> <p>The \\(SEIR\\) model consists of the following compartments:</p> <ul> <li>\\(S\\): Number of individuals that are susceptible to be infected</li> <li>\\(E_1, \\ldots, E_M\\): Number of individuals at different stages of exposure (infected but not yet infectious)</li> <li>\\(I_1, \\ldots, I_K\\): Number of infectious individuals</li> <li>\\(R\\): Number of individuals removed from the pool of susceptible individuals</li> <li>\\(C_I\\): Counter of the total number of individuals that have been infected</li> <li>\\(\\beta(t)\\): Time-dependent transmission rate</li> </ul> <p>The transmission rate \\(\\beta(t)\\) is modeled using B-splines:</p> \\[ \\log \\beta(t) = \\sum_{i=1}^m \\beta_i B_i(t) \\] <p>where \\(\\{B_i (t)\\}_{i=1}^m\\) form a B-spline basis over the time interval \\([t_0, t_1]\\), with \\(m=q+d-1\\) (\\(q\\) is the number of knots, \\(d\\) is the degree of the polynomials of the B-splines); and \\(\\beta=(\\beta_1, \\ldots, \\beta_m)\\) is a vector of coefficients.</p> <p>The \\(SEIR\\) model is governed by the following system of ODEs:</p> \\[ \\begin{align} \\dfrac{dS}{dt} &amp;= -\\beta(t)S(t)\\dfrac{I(t)}{N}\\\\ \\dfrac{dE_1}{dt} &amp;= \\exp \\left(\\sum_{i=1}^m \\beta_i B_i(t)\\right)S(t)\\dfrac{I(t)}{N}-M\\alpha E_1(t)\\\\ \\dfrac{dE_M}{dt} &amp;= M\\alpha E_{M-1}(t) - M\\alpha E_M(t)\\\\ \\dfrac{dI_1}{dt} &amp;= M\\alpha E_M (t) - K\\gamma I_1 (t)\\\\ \\dfrac{dI_K}{dt} &amp;= K \\gamma I_{K-1}(t) - K\\gamma I_K (t)\\\\ \\dfrac{dR}{dt}&amp;=K\\gamma I_K (t)\\\\ \\dfrac{dC_I}{dt} &amp;=\\exp \\left(\\sum_{i=1}^m \\beta_i B_i(t)\\right)S(t)\\dfrac{I(t)}{N} \\end{align} \\] <p>with the following constraints:</p> \\[ \\begin{cases} S(t_0) = N - E_0, E_1(t_0)=C_I (t_0) = E_0\\\\ E_2(t_0)=\\cdots=E_M(t_0)=I_1(t_0)=\\cdots=I_K(t_0)=R(t_0)=0\\\\ E(t) = \\sum_{i=1}^M E_i(t)\\\\ I(t) = \\sum_{j=1}^K I_j(t)\\\\ N = S(t) + E(t) + I(t) + R(t) \\end{cases} \\]"},{"location":"benchmarks/#talbot-physical-effect","title":"Talbot Physical Effect","text":"<p>This benchmark analyzes Partial Differential Equations (PDEs) in the context of the phenomenon occurring when a plane light wave is diffracted by an infinite set of equally spaced slits (the grating, with distance \\(d\\) between the slits).</p> <p>We wish to find solutions to the following differential equation:</p> \\[ \\dfrac{\\partial^2 u}{\\partial t^2}=\\dfrac{\\partial^2 u}{\\partial x^2}+\\dfrac{\\partial^2 u}{\\partial y^2}+\\dfrac{\\partial^2 u}{\\partial z^2} \\] <p>in the domain \\(0 \\leq x \\leq \\frac{d}{2}\\), \\(z \\geq 0\\), \\(t \\geq 0\\) under the border conditions in \\(x\\):</p> \\[ \\dfrac{\\partial u}{\\partial x}=0 \\quad \\text{for} \\quad x=0, \\quad \\dfrac{\\partial u}{\\partial x}=0 \\quad \\text{for} \\quad x=d/2 \\] <p>the boundary conditions in \\(z\\):</p> \\[ u(t, x, z=0)=f(t,x)= \\sin(\\omega t)\\theta(t)\\, \\chi\\left(\\dfrac{x}{w}\\right) \\] <p>and the initial conditions:</p> \\[ u(t=0, x, z) = 0, \\qquad \\dfrac{\\partial u}{\\partial t}(t=0, x, z) = 0 \\] <p>The solution can be expressed in closed-form as:</p> \\[ u(t,x,z)=\\sum_n g_n \\left(\\sin \\omega (t-z) - k_n z \\int_z^t \\dfrac{J_1 (k_n \\sqrt{\\tau^2-z^2})}{\\sqrt{\\tau^2-z^2}}\\sin \\omega (t-\\tau)\\,d{\\tau}\\right)\\theta (t-z) \\cos k_n x \\] <p>Solving the problem entails numerically approximating the complex integral, which involves:</p> <ol> <li>A Bessel function of the first kind.</li> <li>An avoidable singularity as \\(\\tau \\rightarrow z\\).</li> <li>A composition of two highly oscillatory functions.</li> </ol>"},{"location":"quick_start/","title":"Quick Start","text":"<p>In this introduction notebook we provide a simple implementation of a Bayesian Logistic Regression (BLR) model so that users can become familiarized with our library and how to implement their own computational statistics models.</p> <p>First, we begin by importing <code>pyHaiCS</code>. Also, we can check the version running... <pre><code>import pyHaiCS as haics\nprint(f\"Running pyHaiCS v.{haics.__version__}\")\n</code></pre></p>"},{"location":"quick_start/#example-1-bayesian-logistic-regression-for-breast-cancer-classification","title":"Example 1 - Bayesian Logistic Regression for Breast Cancer Classification","text":"<p>As a toy example, we implement below a classic BLR classifier for predicting (binary) breast cancer outcomes... <pre><code>import jax\nimport jax.numpy as jnp\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Standardize the data &amp; convert to jax arrays\nscaler = StandardScaler()\nX_train = jnp.array(scaler.fit_transform(X_train))\nX_test = jnp.array(scaler.transform(X_test))\n\n# Add column of ones to the input data (for intercept terms)\nX_train = jnp.hstack([X_train, jnp.ones((X_train.shape[0], 1))])\nX_test = jnp.hstack([X_test, jnp.ones((X_test.shape[0], 1))])\n</code></pre></p> <p>First, we train a baseline point-estimate logistic regression model... <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef baseline_classifier(X_train, y_train, X_test, y_test):\n    clf = LogisticRegression(random_state = 42)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy (w/ Scikit-Learn Baseline): {accuracy:.3f}\\n\")\n\nbaseline_classifier(X_train, y_train, X_test, y_test)\n</code></pre></p> <p>Now, in order to implement its Bayesian counterpart, we start by writing the model and the Hamiltonian potential (i.e., the negative log-posterior)...</p> <pre><code># Bayesian Logistic Regression model (in JAX)\n@jax.jit\ndef model_fn(x, params):\n    return jax.nn.sigmoid(jnp.matmul(x, params))\n\n@jax.jit\ndef prior_fn(params):\n    return jax.scipy.stats.norm.pdf(params)\n\n@jax.jit\ndef log_prior_fn(params):\n    return jnp.sum(jax.scipy.stats.norm.logpdf(params))\n\n@jax.jit\ndef likelihood_fn(x, y, params):\n    preds = model_fn(x, params)\n    return jnp.prod(preds ** y * (1 - preds) ** (1 - y))\n\n@jax.jit\ndef log_likelihood_fn(x, y, params):\n    epsilon = 1e-7\n    preds = model_fn(x, params)\n    return jnp.sum(y * jnp.log(preds + epsilon) + (1 - y) * jnp.log(1 - preds + epsilon))\n\n@jax.jit\ndef posterior_fn(x, y, params):\n    return prior_fn(params) * likelihood_fn(x, y, params)\n\n@jax.jit\ndef log_posterior_fn(x, y, params):\n    return log_prior_fn(params) + log_likelihood_fn(x, y, params)\n\n@jax.jit\ndef neg_posterior_fn(x, y, params):\n    return -posterior_fn(x, y, params)\n\n# Define a wrapper function to negate the log posterior\n@jax.jit\ndef neg_log_posterior_fn(x, y, params):\n    return -log_posterior_fn(x, y, params)\n</code></pre> <p>Then, we can call the <code>HMC</code> sampler in <code>pyHaiCS</code> (and sample from several chains at once) with a very high-level interface... <pre><code># Initialize the model parameters (includes intercept term)\nkey = jax.random.PRNGKey(42)\nmean_vector = jnp.zeros(X_train.shape[1])\ncov_mat = jnp.eye(X_train.shape[1])\nparams = jax.random.multivariate_normal(key, mean_vector, cov_mat)\n\n# HMC for posterior sampling\nparams_samples = haics.samplers.hamiltonian.HMC(params, \n                            potential_args = (X_train, y_train),\n                            n_samples = 1000, burn_in = 200, \n                            step_size = 1e-3, n_steps = 100, \n                            potential = neg_log_posterior_fn,  \n                            mass_matrix = jnp.eye(X_train.shape[1]), \n                            integrator = haics.integrators.VerletIntegrator(), \n                            RNG_key = 120)\n\n# Average across chains\nparams_samples = jnp.mean(params_samples, axis = 0)\n\n# Make predictions using the samples\npreds = jax.vmap(lambda params: model_fn(X_test, params))(params_samples)\nmean_preds = jnp.mean(preds, axis = 0)\nmean_preds = mean_preds &gt; 0.5\n\n# Evaluate the model\naccuracy = jnp.mean(mean_preds == y_test)\nprint(f\"Accuracy (w/ HMC Sampling): {accuracy}\\n\")\n</code></pre></p>"},{"location":"quick_start/#example-2-sampling-from-a-banana-shaped-distribution","title":"Example 2 - Sampling from a Banana-shaped Distribution","text":"<p>In this example, we will demonstrate how to sample from a banana-shaped distribution using <code>pyHaiCS</code>. This distribution is often used as a benchmark for MCMC algorithms and is defined as follows...</p> <p>Given data \\(\\lbrace y_k\\rbrace_{k=1}^K\\), we sample from the banana-shaped posterior distribution of the parameter \\(\\symbf{\\theta} = (\\theta_1, \\theta_2)\\) for which the likelihood and prior distributions are respectively given as:</p> \\[ \\begin{aligned} y_k|\\symbf{\\theta} &amp;\\sim \\mathcal{N}(\\theta_1+\\theta_2^2, \\sigma_y^2), \\quad k=1, 2, \\ldots, K\\\\ \\theta_1,\\theta_2 &amp;\\sim \\mathcal{N}(0,\\sigma_{\\theta}^2) \\end{aligned} \\] <p>The sample data are generated with \\(\\theta_1+\\theta_2^2=1, \\sigma_y=2, \\sigma_{\\theta}=1\\). Then, the potential is given by:</p> \\[     U(\\symbf{\\theta})=\\dfrac{1}{2\\sigma_y^2}\\sum_{k=1}^K (y_k - \\theta_1 - \\theta_2^2)^2 + \\log \\left(\\sigma_\\theta^2\\sigma_y^{100}\\right)+\\dfrac{1}{2\\sigma_\\theta^2}(\\theta_1^2 + \\theta_2^2) \\] <p>The resulting samples were produced for 10 independent chains, each with 5000 burn-in iterations, 5000 samples, \\(L=14\\) integration steps, a step-size of \\(\\varepsilon=1/9\\), and a momentum noise of \\(\\phi=0.5\\).</p> <p>First, let's generate some synthetic data based on the description. We set \\(\\theta_1+\\theta_2^2=1\\), \\(\\sigma_y=2\\), and \\(\\sigma_{\\theta}=1\\). We will generate \\(K=100\\) data points (provided in the benchmarks), and then estimate the parameter posteriors using HMC or any other sampler.</p> <p><pre><code>filePath = os.path.join(os.path.dirname(__file__), \n                        f\"../pyHaiCS/benchmarks/BNN/Banana_100.txt\")\ny = pd.read_table(filePath, header = None, sep = '\\\\s+').values.reshape(-1)\n\n# Initialize the model parameters\nkey = jax.random.PRNGKey(42)\nkey_HMC, key_GHMC = jax.random.split(key, 2)\nmean_vector = jnp.zeros(2)\ncov_mat = jnp.eye(2)\nparams = jax.random.multivariate_normal(key_HMC, mean_vector, cov_mat)\nsigma_y, sigma_params = 2, 1\n\n# HMC Sampling\nparams_samples_HMC = haics.samplers.hamiltonian.HMC(params, \n                            potential_args = (y, sigma_y, sigma_params),                                           \n                            n_samples = 5000, burn_in = 5000, \n                            step_size = 1/9, n_steps = 14, \n                            potential = potential_fn,  \n                            mass_matrix = jnp.eye(2), \n                            integrator = haics.integrators.VerletIntegrator(), \n                            RNG_key = 120, n_chains = 10)\n</code></pre> where the Hamiltonian potential is defined as follows: <pre><code>@jax.jit\ndef potential_fn(y, sigma_y, sigma_params, params):\n    return 1/(2 * sigma_y ** 2) * jnp.sum((y - params[0] - params[1]**2) ** 2) \\\n             + 1/(2 * sigma_params ** 2) * (params[0] ** 2 + params[1] ** 2)\n</code></pre></p> <p>The results for the first three chains, using HMC &amp; GHMC, are shown below:</p>"}]}