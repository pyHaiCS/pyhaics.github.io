{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to pyHaiCS!","text":"<p>Introducing <code>pyHaiCS</code>, a Python library for Hamiltonian-based Monte-Carlo methods tailored towards practical applications in computational statistics. From sampling complex probability distributions, to approximating complex integrals, <code>pyHaiCS</code> is designed to be fast, flexible, and easy to use, with a focus on providing a user-friendly interface for researchers and practitioners while also offering users a variety of advanced features. </p> <p>Although currently still in development, our library implements a wide range of sampling algorithms \u2014 including single-chain and multi-chain Hamiltoninan Monte-Carlo (HMC) and Generalized HMC (GHMC); a variety of numerical schemes for the integration of the simulated Hamiltonian dynamics (including a generalized version of Multi-Stage Splitting integrators), or a novel adaptive algorithm \u2014 Adaptive Integration Approach in Computational Statistics (s-AIA) \u2014 for the automatic tuning of the parameters of both the numerical integrator and the sampler. </p> <p>Likewise, several utilities for diagnosing the convergence and efficiency of the sampling process, as well as multidisciplinary benchmarks \u2014 ranging from simple toy problems such as sampling from specific distributions, to more complex real-world applications in the fields of computational biology, Bayesian modeling, or physics \u2014 are provided.</p>"},{"location":"#pyhaics-features","title":"pyHaiCS Features","text":"<p>The main features of pyHaiCS, as summarized in the figure below, include its:</p> <ul> <li>Efficient Implementation: <code>pyHaiCS</code> is built on top of the <code>JAX</code> library developed by Google which provides automatic differentiation for computing gradients and Hessians, and Just-In-Time (JIT) compilation for fast numerical computations. Additionally, the library is designed to take advantage of multi-core CPUs, GPUs, or even TPUs for accelerated sampling, and to be highly parallelizable (e.g., by running each chain of multi-chain HMC in a separate CPU core/thread in the GPU).</li> <li>User-Friendly Interface: The library is designed to be easy to use, with a simple and intuitive API that abstracts away the complexities of Hamiltonian Monte-Carlo (HMC) and related algorithms. Users can define their own potential functions and priors, and run sampling algorithms with just a few lines of code.</li> <li>Integration with Existing Tools: The library is designed to be easily integrated with other Python libraries, such as <code>NumPy</code>, <code>SciPy</code>, and <code>Scikit-Learn</code>. This allows users to leverage existing tools and workflows, and build on top of the rich ecosystem of scientific computing in Python. Therefore, users can easily incorporate <code>pyHaiCS</code> into their existing Machine Learning workflows, and use it for tasks such as inference, model selection, or parameter estimation in the context of Bayesian modeling.</li> <li>Advanced Features: <code>pyHaiCS</code> supports a variety of Hamiltonian-inspired sampling algorithms, including single-chain and multi-chain HMC (and GHMC), generalized \\(k\\)-th stage Multi-Stage Splitting integrators, and adaptive integration schemes (such as s-AIA).</li> </ul> <p>In order to provide a functional and easy-to-use library, and especially to ensure that our code can be easily integrated into existing workflows, we have designed <code>pyHaiCS</code> with a simple rule in mind: Objects are specified by interface, not by inheritance. That is, much alike <code>Scikit-Learn</code>, inheritance is not enforced; and instead, code conventions provide a consistent interface for all samplers, integrators, and utilities. This allows for a more flexible and modular design, and makes it easier for users to extend the library with their own custom implementations. As Scikit's design around making all estimators have a consistent <code>fit</code> and <code>predict</code> interface, <code>pyHaiCS</code> follows a similar approach, but with a focus on Hamiltonian Monte-Carlo methods and its related algorithms. For instance, all integrators in <code>pyHaiCS</code> have a consistent <code>integrate</code> method, which takes as input the potential function, the initial state, and the parameters of the integrator, and returns the final state of the system after the integration process. This consistent interface makes it easy for users to switch between different integrators, or to implement their own custom integrators, without having to worry about the underlying details of the implementation. </p> <p>Moreover, <code>pyHaiCS</code> is designed to be highly modular, with each component of the library being self-contained and independent of the others, as well as being easily extensible and customizable. As a further point of strength, our library handles all auto-differentiation (such as potential gradients and Hessians) through the <code>JAX</code> library, which provides a fast and efficient way to compute gradients as well as a higher level of abstraction for the user to focus on the actual problem at hand. By only defining the potential function of the Hamiltonian, the user can easily run the sampler and obtain the posterior distribution of the parameters of interest. As an example of the ease-of-use of <code>pyHaiCS</code>, we show below a simple example of how to define a Bayesian Logistic Regression (BLR) model:</p> <pre><code># Step 1 - Define the BLR model\n@jax.jit\ndef model_fn(x, params):\n    return jax.nn.sigmoid(jnp.matmul(x, params))\n\n# Step 2 - Define the log-prior and log-likelihood\n@jax.jit\ndef log_prior_fn(params):\n    return jnp.sum(jax.scipy.stats.norm.logpdf(params))\n\n@jax.jit\ndef log_likelihood_fn(x, y, params):\n    preds = model_fn(x, params)\n    return jnp.sum(y * jnp.log(preds) + (1 - y) * jnp.log(1 - preds))\n\n# Step 3 - Define the log-posterior (remember, the oppositve of the potential)\n@jax.jit\ndef log_posterior_fn(x, y, params):\n    return log_prior_fn(params) + log_likelihood_fn(x, y, params)\n\n# Initialize the model parameters (including intercept term)\nkey = jax.random.PRNGKey(42)\nmean_vector, cov_mat = jnp.zeros(X_train.shape[1]), jnp.eye(X_train.shape[1])\nparams = jax.random.multivariate_normal(key, mean_vector, cov_mat)\n\n# HMC for posterior sampling\nparams_samples = haics.samplers.hamiltonian.HMC(params, \n                            potential_args = (X_train, y_train),\n                            n_samples = 1000, burn_in = 200, \n                            step_size = 1e-3, n_steps = 100, \n                            potential = neg_log_posterior_fn,  \n                            mass_matrix = jnp.eye(X_train.shape[1]), \n                            integrator = haics.integrators.VerletIntegrator(), \n                            RNG_key = key)\n\n# Average across chains\nparams_samples = jnp.mean(params_samples, axis = 0)\n\n# Make predictions using the samples\npreds = jax.vmap(lambda params: model_fn(X_test, params))(params_samples)\nmean_preds = jnp.mean(preds, axis = 0)\n</code></pre> <p>Regarding the actual features implemented in <code>pyHaiCS</code>, and the general organization of its API, the figure below provides a high-level overview of the main components of the library. As can be seen, the library is organized around four main components: Hamiltonian Samplers, Numerical Integrators, Adaptive Tuning, and Sampling Metrics. Each of these components is further divided into sub-components, such as the different samplers implemented in the library (e.g., HMC, GHMC, and the yet to be implemented, MMHMC), the numerical integrators (such as variants of Velocity-Verlet, and  2-Stage and 3-Stage MSSIs), or the s-AIA adaptive tuning scheme. The library also includes a variety of sampling metrics for diagnosing the convergence and efficiency of the sampling process, as well as multidisciplinary benchmarks (and code examples) for testing the performance of the library.</p>"},{"location":"#introduction-to-hamiltonian-monte-carlo","title":"Introduction to Hamiltonian Monte-Carlo","text":"<p>Markov-Chain Monte-Carlo (MCMC) methods are powerful tools for sampling from complex probability distributions, a task that lies at the heart of many statistical and Machine Learning problems. Among these, Hamiltonian Monte-Carlo (HMC) stands out as a particularly efficient and versatile algorithm, especially well-suited for high-dimensional problems.</p> <p>Traditional MCMC methods, such as the Metropolis-Hastings algorithm or Gibbs sampling, often rely on random walk behavior to explore the target distribution. While effective, this can lead to slow convergence, especially when dealing with complex, multimodal, or high-dimensional distributions.  HMC addresses these limitations by introducing concepts from Hamiltonian dynamics to guide the exploration of the sample space.</p> <p>At its core, HMC leverages the idea of simulating the movement of a particle in a physical system to generate efficient transitions across the target distribution \\(\\pi(\\mathbf{q})\\).  Let's break down the key elements:</p> <ul> <li> <p>Augmenting the State Space:  Imagine the probability distribution we want to sample from \u2013 our target distribution \u2013 as defining a potential energy landscape. Regions of high probability correspond to valleys (low potential energy), while regions of low probability are hills (high potential energy). To introduce dynamics, HMC augments our state space by adding auxiliary momentum variables, typically denoted as \\(\\mathbf{p}\\), for each position variable \\(\\mathbf{q}\\) (our original parameters of interest).</p> </li> <li> <p>Hamiltonian Function: We then define a Hamiltonian function, \\(H(\\mathbf{q}, \\mathbf{p})\\), which describes the total energy of the system.  This function is typically the sum of two components:</p> <ul> <li>Potential Energy, \\(U(\\mathbf{q})\\):  This is directly related to our target probability distribution, \\(\\pi(\\mathbf{q})\\). Specifically, we often set \\(U(\\mathbf{q}) = -\\log \\pi(\\mathbf{q})\\).  Minimizing the potential energy corresponds to finding regions of high probability under \\(\\pi(\\mathbf{q})\\).</li> <li>Kinetic Energy, \\(K(\\mathbf{p})\\): This term depends on the momentum variables and is usually defined as the energy of a fictitious \"particle\" associated with our system. A common choice is the kinetic energy of a particle with unit mass: \\(K(\\mathbf{p}) = \\frac{1}{2} \\mathbf{p}^T \\mathbf{M}^{-1} \\mathbf{p}\\), where \\(\\mathbf{M}\\) is a mass matrix (often set to the identity matrix for simplicity).</li> </ul> <p>The Hamiltonian is then \\(H(\\mathbf{q}, \\mathbf{p}) = U(\\mathbf{q}) + K(\\mathbf{p}) = -\\log \\pi(\\mathbf{q}) + \\frac{1}{2} \\mathbf{p}^T \\mathbf{M}^{-1} \\mathbf{p}\\).</p> </li> <li> <p>Hamilton's Equations of Motion:  The dynamics of the system are governed by Hamilton's equations of motion. These equations describe how the positions and momenta evolve over time:</p> \\[ \\begin{aligned} \\frac{d\\mathbf{q}}{dt} &amp;= \\frac{\\partial H}{\\partial \\mathbf{p}} = \\mathbf{M}^{-1} \\mathbf{p} \\\\ \\frac{d\\mathbf{p}}{dt} &amp;= -\\frac{\\partial H}{\\partial \\mathbf{q}} = -\\frac{\\partial U}{\\partial \\mathbf{q}} = \\nabla \\log \\pi(\\mathbf{q}) \\end{aligned} \\] <p>These equations dictate that the \"particle\" will move through the potential energy landscape. Crucially, under these dynamics, the Hamiltonian \\(H(\\mathbf{q}, \\mathbf{p})\\) (and thus the density \\(\\propto \\exp(-H(\\mathbf{q}, \\mathbf{p}))\\)) remains constant over time in a continuous system.</p> </li> <li> <p>Numerical Integration:  To simulate these dynamics on a computer, we need to discretize time and use a numerical integrator.  <code>pyHaiCS</code> offers a variety of numerical integrators, including symplectic integrators, which are particularly well-suited for Hamiltonian systems because they preserve important properties of the dynamics, such as volume preservation and near-conservation of energy.</p> </li> <li> <p>Metropolis Acceptance Step:  While the Hamiltonian dynamics ideally preserve the target distribution, numerical integration introduces approximations, and thus trajectories are not perfectly Hamiltonian. To correct for these errors and ensure we are still sampling from the exact target distribution, HMC incorporates a Metropolis acceptance step. After evolving the system for a certain time using numerical integration, we compute the change in Hamiltonian, \\(\\Delta H\\), between the start and end points of the trajectory.  We then accept the proposed new state with probability:</p> \\[ \\alpha = \\min\\left(1, \\exp(-\\Delta H)\\right) = \\min\\left(1, \\exp(H(\\mathbf{q}_{old}, \\mathbf{p}_{old}) - H(\\mathbf{q}_{new}, \\mathbf{p}_{new}))\\right) \\] <p>If the proposal is rejected, we simply retain the previous state. This acceptance step guarantees that the HMC algorithm samples from the correct target distribution, even with numerical integration approximations.</p> </li> </ul> <p>Benefits of HMC:</p> <p>By leveraging Hamiltonian dynamics, HMC offers several advantages over traditional MCMC methods:</p> <ul> <li>Efficient Exploration of State Space: The Hamiltonian dynamics allows for more directed and less random-walk-like exploration of the target distribution.  Trajectories tend to follow gradients of the potential energy, enabling the sampler to move more quickly across the state space, especially in high dimensions.</li> <li>Reduced Random Walk Behavior:  Unlike algorithms relying on random proposals, HMC trajectories can travel significant distances in state space in each step, leading to faster convergence and more efficient sampling, particularly for distributions with complex geometries or long, narrow regions of high probability.</li> <li>Scalability to High Dimensions: The efficiency gains of HMC become more pronounced as the dimensionality of the problem increases, making it a powerful tool for complex statistical models with many parameters.</li> </ul> <p>In summary, Hamiltonian Monte Carlo provides a robust and efficient approach to MCMC sampling by harnessing the principles of Hamiltonian dynamics. By carefully simulating the physical movement of a system guided by the target distribution, HMC overcomes many limitations of traditional MCMC methods, enabling faster and more reliable sampling from complex, high-dimensional probability distributions.  <code>pyHaiCS</code> aims to make these powerful methods accessible and easy to use for a wide range of applications in computational statistics and beyond :)</p>"},{"location":"api/","title":"API","text":"<p>To be added soon :)</p>"},{"location":"benchmarks/","title":"Experimental Models","text":"<p>To be added soon :)</p>"},{"location":"quick_start/","title":"Quick Start","text":"<p>In this introduction notebook we provide a simple implementation of a Bayesian Logistic Regression (BLR) model so that users can become familiarized with our library and how to implement their own computational statistics models.</p> <p>First, we begin by importing <code>pyHaiCS</code>. Also, we can check the version running... <pre><code>import pyHaiCS as haics\nprint(f\"Running pyHaiCS v.{haics.__version__}\")\n</code></pre></p>"},{"location":"quick_start/#example-1-bayesian-logistic-regression-for-breast-cancer-classification","title":"Example 1 - Bayesian Logistic Regression for Breast Cancer Classification","text":"<p>As a toy example, we implement below a classic BLR classifier for predicting (binary) breast cancer outcomes... <pre><code>import jax\nimport jax.numpy as jnp\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Standardize the data &amp; convert to jax arrays\nscaler = StandardScaler()\nX_train = jnp.array(scaler.fit_transform(X_train))\nX_test = jnp.array(scaler.transform(X_test))\n\n# Add column of ones to the input data (for intercept terms)\nX_train = jnp.hstack([X_train, jnp.ones((X_train.shape[0], 1))])\nX_test = jnp.hstack([X_test, jnp.ones((X_test.shape[0], 1))])\n</code></pre></p> <p>First, we train a baseline point-estimate logistic regression model... <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef baseline_classifier(X_train, y_train, X_test, y_test):\n    clf = LogisticRegression(random_state = 42)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy (w/ Scikit-Learn Baseline): {accuracy:.3f}\\n\")\n\nbaseline_classifier(X_train, y_train, X_test, y_test)\n</code></pre></p> <p>Now, in order to implement its Bayesian counterpart, we start by writing the model and the Hamiltonian potential (i.e., the negative log-posterior)...</p> <pre><code># Bayesian Logistic Regression model (in JAX)\n@jax.jit\ndef model_fn(x, params):\n    return jax.nn.sigmoid(jnp.matmul(x, params))\n\n@jax.jit\ndef prior_fn(params):\n    return jax.scipy.stats.norm.pdf(params)\n\n@jax.jit\ndef log_prior_fn(params):\n    return jnp.sum(jax.scipy.stats.norm.logpdf(params))\n\n@jax.jit\ndef likelihood_fn(x, y, params):\n    preds = model_fn(x, params)\n    return jnp.prod(preds ** y * (1 - preds) ** (1 - y))\n\n@jax.jit\ndef log_likelihood_fn(x, y, params):\n    epsilon = 1e-7\n    preds = model_fn(x, params)\n    return jnp.sum(y * jnp.log(preds + epsilon) + (1 - y) * jnp.log(1 - preds + epsilon))\n\n@jax.jit\ndef posterior_fn(x, y, params):\n    return prior_fn(params) * likelihood_fn(x, y, params)\n\n@jax.jit\ndef log_posterior_fn(x, y, params):\n    return log_prior_fn(params) + log_likelihood_fn(x, y, params)\n\n@jax.jit\ndef neg_posterior_fn(x, y, params):\n    return -posterior_fn(x, y, params)\n\n# Define a wrapper function to negate the log posterior\n@jax.jit\ndef neg_log_posterior_fn(x, y, params):\n    return -log_posterior_fn(x, y, params)\n</code></pre> <p>Then, we can call the <code>HMC</code> sampler in <code>pyHaiCS</code> (and sample from several chains at once) with a very high-level interface... <pre><code># Initialize the model parameters (includes intercept term)\nkey = jax.random.PRNGKey(42)\nmean_vector = jnp.zeros(X_train.shape[1])\ncov_mat = jnp.eye(X_train.shape[1])\nparams = jax.random.multivariate_normal(key, mean_vector, cov_mat)\n\n# HMC for posterior sampling\nparams_samples = haics.samplers.hamiltonian.HMC(params, \n                            potential_args = (X_train, y_train),\n                            n_samples = 1000, burn_in = 200, \n                            step_size = 1e-3, n_steps = 100, \n                            potential = neg_log_posterior_fn,  \n                            mass_matrix = jnp.eye(X_train.shape[1]), \n                            integrator = haics.integrators.VerletIntegrator(), \n                            RNG_key = 120)\n\n# Average across chains\nparams_samples = jnp.mean(params_samples, axis = 0)\n\n# Make predictions using the samples\npreds = jax.vmap(lambda params: model_fn(X_test, params))(params_samples)\nmean_preds = jnp.mean(preds, axis = 0)\nmean_preds = mean_preds &gt; 0.5\n\n# Evaluate the model\naccuracy = jnp.mean(mean_preds == y_test)\nprint(f\"Accuracy (w/ HMC Sampling): {accuracy}\\n\")\n</code></pre></p>"},{"location":"quick_start/#example-2-sampling-from-a-banana-shaped-distribution","title":"Example 2 - Sampling from a Banana-shaped Distribution","text":"<p>In this example, we will demonstrate how to sample from a banana-shaped distribution using <code>pyHaiCS</code>. This distribution is often used as a benchmark for MCMC algorithms and is defined as follows...</p> <p>Given data \\(\\lbrace y_k\\rbrace_{k=1}^K\\), we sample from the banana-shaped posterior distribution of the parameter \\(\\symbf{\\theta} = (\\theta_1, \\theta_2)\\) for which the likelihood and prior distributions are respectively given as:</p> \\[ \\begin{aligned} y_k|\\symbf{\\theta} &amp;\\sim \\mathcal{N}(\\theta_1+\\theta_2^2, \\sigma_y^2), \\quad k=1, 2, \\ldots, K\\\\ \\theta_1,\\theta_2 &amp;\\sim \\mathcal{N}(0,\\sigma_{\\theta}^2) \\end{aligned} \\] <p>The sample data are generated with \\(\\theta_1+\\theta_2^2=1, \\sigma_y=2, \\sigma_{\\theta}=1\\). Then, the potential is given by:</p> \\[     U(\\symbf{\\theta})=\\dfrac{1}{2\\sigma_y^2}\\sum_{k=1}^K (y_k - \\theta_1 - \\theta_2^2)^2 + \\log \\left(\\sigma_\\theta^2\\sigma_y^{100}\\right)+\\dfrac{1}{2\\sigma_\\theta^2}(\\theta_1^2 + \\theta_2^2) \\] <p>The resulting samples were produced for 10 independent chains, each with 5000 burn-in iterations, 5000 samples, \\(L=14\\) integration steps, a step-size of \\(\\varepsilon=1/9\\), and a momentum noise of \\(\\phi=0.5\\).</p> <p>First, let's generate some synthetic data based on the description. We set \\(\\theta_1+\\theta_2^2=1\\), \\(\\sigma_y=2\\), and \\(\\sigma_{\\theta}=1\\). We will generate \\(K=100\\) data points (provided in the benchmarks), and then estimate the parameter posteriors using HMC or any other sampler.</p> <p><pre><code>filePath = os.path.join(os.path.dirname(__file__), \n                        f\"../pyHaiCS/benchmarks/BNN/Banana_100.txt\")\ny = pd.read_table(filePath, header = None, sep = '\\\\s+').values.reshape(-1)\n\n# Initialize the model parameters\nkey = jax.random.PRNGKey(42)\nkey_HMC, key_GHMC = jax.random.split(key, 2)\nmean_vector = jnp.zeros(2)\ncov_mat = jnp.eye(2)\nparams = jax.random.multivariate_normal(key_HMC, mean_vector, cov_mat)\nsigma_y, sigma_params = 2, 1\n\n# HMC Sampling\nparams_samples_HMC = haics.samplers.hamiltonian.HMC(params, \n                            potential_args = (y, sigma_y, sigma_params),                                           \n                            n_samples = 5000, burn_in = 5000, \n                            step_size = 1/9, n_steps = 14, \n                            potential = potential_fn,  \n                            mass_matrix = jnp.eye(2), \n                            integrator = haics.integrators.VerletIntegrator(), \n                            RNG_key = 120, n_chains = 10)\n</code></pre> where the Hamiltonian potential is defined as follows: <pre><code>@jax.jit\ndef potential_fn(y, sigma_y, sigma_params, params):\n    return 1/(2 * sigma_y ** 2) * jnp.sum((y - params[0] - params[1]**2) ** 2) \\\n             + 1/(2 * sigma_params ** 2) * (params[0] ** 2 + params[1] ** 2)\n</code></pre></p> <p>The results for the first three chains, using HMC &amp; GHMC, are shown below:</p>"}]}